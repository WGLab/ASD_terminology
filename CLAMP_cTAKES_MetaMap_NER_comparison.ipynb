{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the formatted CLAMP, cTAKES, and MetaMap output/predictions and runs performance statistics on them compared to the labels generated with the benchmark (BM) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'statistics' already exists, so a new folder was not created.\n"
     ]
    }
   ],
   "source": [
    "# configurations that can be modified\n",
    "ABSTRACT = True # True if running this program of abstracts, False if running on full-texts\n",
    "FILTER = False # True to filter some predicted terms to increase precision, False to use raw predictions/output\n",
    "\n",
    "RELEVANT_TUIS_FULLTEXT = ['T033', 'T048']\n",
    "RELEVANT_TUIS_ABSTRACT = ['T033', 'T048']\n",
    "\n",
    "# for naming files\n",
    "if FILTER:\n",
    "    filtered = \"filtered_\"\n",
    "else:\n",
    "    filtered = \"\"\n",
    "    \n",
    "STATS_DIR = \"statistics\" # directory where performance statistics will be saved\n",
    "\n",
    "if not os.path.exists(STATS_DIR):\n",
    "    os.makedirs(STATS_DIR)\n",
    "else:\n",
    "    print(f\"The folder '{STATS_DIR}' already exists, so a new folder was not created.\")\n",
    "    \n",
    "\n",
    "# location of full-texts/abstracts in plain text\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_fulltexts_544\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_20408\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and format CUI\n",
    "def get_CUI(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    else:\n",
    "        return x.split()[0].strip()\n",
    "    \n",
    "\n",
    "# function for filtering predictions\n",
    "def filter_pred(pred_df_temp, remove_non_asd=False, filter_by_cui=True, clamp_problem=False):\n",
    "    \n",
    "    print(\"Filtering\")\n",
    "    \n",
    "    if filter_by_cui:\n",
    "        pred_df_temp = pred_df_temp.dropna(subset=[\"CUI\"]) # keep only terms with CUI\n",
    "        pred_df_temp = pred_df_temp[(pred_df_temp[\"CUI\"].str.len() == 8) & (pred_df_temp[\"CUI\"].str[0] == 'C')] # valid CUI only\n",
    "        if ABSTRACT:\n",
    "            RELEVANT_TUIS = RELEVANT_TUIS_ABSTRACT\n",
    "        else:\n",
    "            RELEVANT_TUIS = RELEVANT_TUIS_FULLTEXT\n",
    "        pred_df_temp =  pred_df_temp[(pred_df_temp[\"TUI\"].isin(RELEVANT_TUIS)) | (pred_df_temp[\"CUI\"]=='C0018817')] # C0018817 is atrial septal defect\n",
    "    \n",
    "    if clamp_problem:\n",
    "        pred_df_temp = pred_df_temp[pred_df_temp[\"Semantic\"]==\"problem\"]\n",
    "    \n",
    "    # remove non-ASD specific terms (i.e. commorbidities)\n",
    "    if remove_non_asd:\n",
    "        autism_comorbid = set(pd.read_csv(\"asd_psychiatric_commorbidities.csv\")[\"CUI\"])\n",
    "        pred_df_temp = pred_df_temp[~(pred_df_temp[\"CUI\"].isin(autism_comorbid))]\n",
    "        \n",
    "    pred_df = pred_df_temp\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "def calculate_statistics(pred_df, true_df, match_cui=False, match_general=False, match_lenient=False, true_pos_df=None):\n",
    "        \n",
    "    if match_lenient:\n",
    "        true_df = true_df.append(pred_df[(pred_df[\"CUI\"].isin(set(true_df[\"CUI\"])))|(pred_df[\"Entity_lower\"]).str.contains(\"|\".join(set(true_df[\"Entity_lower\"])))])\n",
    "        \n",
    "    # count overlap and CUI match\n",
    "    if match_cui:\n",
    "        print(\"Matching CUI\")\n",
    "        # drop duplicate predictions on same entity span and CUI\n",
    "        pred_df = pred_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"]) # choose one CUI per entity\n",
    "        true_df = true_df.drop_duplicates(subset=[\"paper\", \"CUI\", \"Start\", \"End\"])\n",
    "        true_df = true_df[(true_df[\"CUI\"].str.len() == 8) & (true_df[\"CUI\"].str[0] == 'C')] # valid CUI only\n",
    "        match_grouped = pred_df.merge(true_df, on=[\"paper\", \"CUI\"], how=\"outer\")\n",
    "                \n",
    "    # count overlap only - get true positives\n",
    "    else:\n",
    "        # drop duplicate predictions on same entity span\n",
    "        pred_df = pred_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "        true_df = true_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "        match_grouped = pred_df.merge(true_df, on=[\"paper\"], how=\"outer\")\n",
    "        \n",
    "    match_grouped = match_grouped.rename(columns={\"Start_x\": \"Start_pred\", \"End_x\": \"End_pred\", \"Start_y\": \"Start_label\", \"End_y\": \"End_label\", \"Entity_x\": \"Entity_pred\", \"Entity_y\": \"Entity_label\"})\n",
    "    match_grouped = match_grouped.fillna(\"NA\")\n",
    "    \n",
    "    # count overlaps\n",
    "    if not true_pos_df:\n",
    "        temp = match_grouped[(match_grouped[\"Start_pred\"] != \"NA\") & (match_grouped[\"Start_label\"] != \"NA\")]\n",
    "        temp = temp[((temp[\"Start_pred\"] >= temp[\"Start_label\"]) & (temp[\"Start_pred\"] <= temp[\"End_label\"])) | ((temp[\"Start_label\"] >= temp[\"Start_pred\"]) & (temp[\"Start_label\"] <= temp[\"End_pred\"]))]   \n",
    "        true_pos_df = temp\n",
    "    else:\n",
    "        temp = true_pos_df\n",
    "    \n",
    "    # only count general ASD/behavioral terms\n",
    "    if match_general:\n",
    "        temp = temp[temp[\"TYPE\"]==\"General\"]\n",
    "        true_df = true_df[true_df[\"TYPE\"]==\"General\"]\n",
    "        num_label_pos = len(true_df)\n",
    "        pred_df = pred_df.merge(true_pos_df[true_pos_df[\"TYPE\"]!=\"General\"][[\"paper\", \"Start_pred\", \"End_pred\"]], left_on=[\"paper\", \"Start\", \"End\"], right_on=[\"paper\", \"Start_pred\", \"End_pred\"], how=\"outer\")\n",
    "        pred_df = pred_df[pred_df[\"Start_pred\"].isnull()].drop([\"Start_pred\", \"End_pred\"], axis=1)\n",
    "        num_pred_pos = len(pred_df)\n",
    "        true_pos_df = temp\n",
    "        num_true_pos = len(temp.drop_duplicates([\"paper\", \"Start_label\", \"End_label\"])) # only count max one pred per label\n",
    "    elif match_cui:\n",
    "        num_true_pos = len(temp.drop_duplicates([\"paper\", \"Start_label\", \"End_label\", \"CUI\"])) # only count max one pred per label\n",
    "        num_label_pos = len(true_df)\n",
    "        num_pred_pos = len(pred_df)\n",
    "    else:\n",
    "        num_true_pos = len(temp.drop_duplicates([\"paper\", \"Start_label\", \"End_label\"])) # only count max one pred per label\n",
    "        num_label_pos = len(true_df)\n",
    "        num_pred_pos = len(pred_df)\n",
    "\n",
    "    print(\"Number of true positives =\", num_true_pos)\n",
    "    print(\"Number of positive labels =\", num_label_pos)\n",
    "    print(\"Number of positive predictions =\", num_pred_pos)\n",
    "    print()\n",
    "    precision = num_true_pos/num_pred_pos\n",
    "    recall = num_true_pos/num_label_pos\n",
    "    print(\"Precision =\", precision)\n",
    "    print(\"Recall =\", recall)\n",
    "    print(\"F-Measure =\", (2 * precision * recall) / (precision + recall))\n",
    "    \n",
    "    return true_pos_df, pred_df, true_df\n",
    "\n",
    "\n",
    "# get true positives, false positives, and false negatives\n",
    "def get_false_and_true_pos(true_pos_df, pred_df, true_df, cui=True):\n",
    "    \n",
    "    if cui:\n",
    "        pred_df = pred_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\", \"CUI\"])\n",
    "        true_df = true_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\", \"CUI\"])\n",
    "    else:\n",
    "        pred_df = pred_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "        true_df = true_df.drop_duplicates(subset=[\"paper\", \"Start\", \"End\"])\n",
    "    \n",
    "    # group overlapping entities in true pos df\n",
    "    temp = pd.DataFrame(true_pos_df.groupby(by=[\"Entity_label\", \"Entity_pred\"])[\"Start_pred\"].count()).reset_index()\n",
    "    grouped = pd.DataFrame(temp.groupby(by=[\"Entity_label\"])[\"Start_pred\"].sum()).sort_values(by=\"Start_pred\", ascending=False)\n",
    "    temp = temp.merge(grouped, on=\"Entity_label\")\n",
    "    temp.columns = [\"Entity_label\", \"Entity_pred\", \"Entity_pred count\", \"Entity_label count\"]\n",
    "    temp = temp.sort_values(by=[\"Entity_label count\", \"Entity_pred count\"], ascending=False)\n",
    "    true_pos_grouped = temp\n",
    "    \n",
    "    if cui:\n",
    "        columns = [\"Entity\", \"CUI\", \"TUI\"]\n",
    "    else:\n",
    "        columns = [\"Entity\"]\n",
    "    \n",
    "    # false positives - count overlap as match\n",
    "    temp = pred_df.merge(true_pos_df[[\"paper\", \"Start_pred\", \"End_pred\"]], left_on=[\"paper\", \"Start\", \"End\"], right_on=[\"paper\", \"Start_pred\", \"End_pred\"], how=\"outer\")\n",
    "    \n",
    "    false_pos = temp[temp[\"Start_pred\"].isnull()].sort_values(by=[\"paper\", \"Entity\"])\n",
    "    false_pos_grouped = false_pos.groupby(by=columns)[\"Start\"].count().reset_index().sort_values(by=\"Start\", ascending=False).reset_index(drop=True)\n",
    "    false_pos_grouped = false_pos_grouped.rename(columns={\"Start\":\"count\"})\n",
    "    \n",
    "    # false negative - count overlap as match\n",
    "    temp = true_df.merge(true_pos_df[[\"paper\", \"Start_label\", \"End_label\"]], left_on=[\"paper\", \"Start\", \"End\"], right_on=[\"paper\", \"Start_label\", \"End_label\"], how=\"outer\").drop_duplicates([\"paper\", \"Start\", \"End\"])\n",
    "    false_neg = temp[temp[\"Start_label\"].isnull()].sort_values(by=[\"paper\", \"Entity\"])\n",
    "    false_neg_grouped = false_neg.groupby(by=columns)[\"Start\"].count().reset_index().sort_values(by=\"Start\", ascending=False).reset_index(drop=True)\n",
    "    false_neg_grouped = false_neg_grouped.rename(columns={\"Start\":\"count\"})\n",
    "    \n",
    "    return true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataframe with \"true\" benchmark (BM) labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_DIR = \"BM_labelled\" # folder where the results (dataframe with labels) will be stored\n",
    "\n",
    "if ABSTRACT:\n",
    "    labels_df = pd.read_csv(os.path.join(BM_DIR, \"abstract_labels.csv\"))\n",
    "else:\n",
    "    labels_df = pd.read_csv(os.path.join(BM_DIR, \"full_text_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in BM ASD terms and add TUI\n",
    "BM_df = pd.read_csv(\"BM_terms.csv\")\n",
    "BM_df.rename(columns={\"CUI\": \"CUI_original\"}, inplace=True)\n",
    "BM_df[\"NEGATED\"] = BM_df[\"CUI_original\"].apply(lambda x: str(x)[0] == \"-\")\n",
    "BM_df[\"CUI\"] = BM_df[\"CUI_original\"].apply(lambda x: str(x).replace(\"-\", \"\"))\n",
    "BM_cui_to_tui_df = pd.read_csv(\"tui_list_BM.txt\", sep=\"\\t\", index_col=0, header=None).reset_index()\n",
    "BM_cui_to_tui_df.columns = [\"CUI\", \"TUI\"]\n",
    "BM_df = BM_df.merge(BM_cui_to_tui_df, how=\"left\")\n",
    "BM_df[\"TEXT\"] = BM_df[\"TEXT\"].str.strip().str.lower()\n",
    "BM_df = BM_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge labels with BM term information\n",
    "labels_df = labels_df.merge(BM_df, left_on=\"Entity_lower\", right_on=\"TEXT\", how=\"left\")\n",
    "\n",
    "# clean-up\n",
    "labels_df = labels_df.replace({'Entity_lower': {\"asperger 's\": \"asperger's\"}})\n",
    "labels_df = labels_df.replace({'Entity': {\"asperger 's\": \"asperger's\"}})\n",
    "labels_df = labels_df.replace({'Entity': {\"Asperger 's\": \"Asperger's\"}})\n",
    "\n",
    "# case-sensitive for ASD and ASDs\n",
    "labels_df = labels_df[~((labels_df[\"Entity_lower\"]==\"asds\")&(labels_df[\"Entity\"]!=\"ASDs\"))]\n",
    "labels_df = labels_df[~((labels_df[\"Entity_lower\"]==\"asd\")&(labels_df[\"Entity\"]!=\"ASD\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct true entities detected (case-sensitive): 159\n",
      "Distinct true entities detected (case-insensitive): 106\n"
     ]
    }
   ],
   "source": [
    "# check that entitiy and CUI columns are not empty\n",
    "assert len(labels_df[labels_df[\"Entity\"].str.lower() == \"nan\"]) == 0\n",
    "assert len(labels_df[labels_df[\"Entity_lower\"].str.lower() == \"nan\"]) == 0\n",
    "assert len(labels_df[labels_df[\"CUI\"].str.len() == 0]) == 0\n",
    "\n",
    "print(\"Distinct true entities detected (case-sensitive):\", len(set(labels_df[\"Entity\"])))\n",
    "print(\"Distinct true entities detected (case-insensitive):\", len(set(labels_df[\"Entity_lower\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101 unique CUI\n"
     ]
    }
   ],
   "source": [
    "ASD_CUI = set(BM_df[\"CUI\"])\n",
    "print(f\"There are {len(ASD_CUI)} unique CUI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_original_df = labels_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Manually run cells for CLAMP, cTAKES, or MetaMap anlaysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAMP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations that can be modified\n",
    "CLAMP_DIRECTORY = \"clamp\" # parent directory for CLAMP-related files\n",
    "\n",
    "# formatted CLAMP output/predictions\n",
    "CLAMP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CLAMP_DIRECTORY, \"clamp_results_full_text\")\n",
    "CLAMP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CLAMP_DIRECTORY, \"clamp_results_abstract\")\n",
    "\n",
    "if ABSTRACT:\n",
    "    CLAMP_RESULTS_DIRECTORY = CLAMP_RESULTS_DIRECTORY_ABSTRACT \n",
    "else:\n",
    "    CLAMP_RESULTS_DIRECTORY = CLAMP_RESULTS_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format CLAMP output/predictions\n",
    "pred_df_temp = pd.read_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, \"clamp_preds.csv\"))\n",
    "pred_df_temp[\"CUI\"] = pred_df_temp[\"CUI\"].apply(lambda x: get_CUI(x)) # get CUI\n",
    "\n",
    "# formatting\n",
    "pred_df_temp[\"Entity\"] = pred_df_temp[\"Entity_matched\"].str.strip() # strip entity\n",
    "pred_df_temp[\"Entity_lower\"] = pred_df_temp[\"Entity\"].str.strip().str.lower()\n",
    "\n",
    "# map CUI to TUI\n",
    "cui_to_tui_df = pd.read_csv(\"clamp_cui_to_tui_map.txt\", sep=\"\\t\", header = None)\n",
    "cui_to_tui_df.columns = [\"CUI\", \"TUI\"]\n",
    "pred_df_temp = pred_df_temp.merge(cui_to_tui_df, on=\"CUI\", how=\"left\")\n",
    "\n",
    "pred_df_temp = pred_df_temp[~(pred_df_temp[\"paper\"].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with extra predictions: 511\n"
     ]
    }
   ],
   "source": [
    "if not ABSTRACT:\n",
    "    assert (len((set(labels_df[\"paper\"])).difference(set(pred_df_temp[\"paper\"])))) == 0 # all papers were analyzed\n",
    "print(\"Number of papers with extra predictions:\", len((set(pred_df_temp[\"paper\"])).difference(set(labels_df[\"paper\"])))) # number of papers with extra predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter predictions\n",
    "if FILTER:\n",
    "    pred_df_temp = filter_pred(pred_df_temp, remove_non_asd=True, clamp_problem=False)\n",
    "pred_df_temp = pred_df_temp.drop([\"Semantic\", \"Assertion\", \"Entity_matched\"], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropping duplicates: 382703\n",
      "after dropping duplicates: 370912\n"
     ]
    }
   ],
   "source": [
    "print(\"before dropping duplicates:\", len(pred_df_temp))\n",
    "pred_df_temp = pred_df_temp.drop_duplicates([\"Start\", \"End\", \"paper\", \"CUI\"])\n",
    "print(\"after dropping duplicates:\", len(pred_df_temp))\n",
    "pred_df_clamp = pred_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMP mean entity no. of words = 2.3123290184376804\n",
      "CLAMP std entity no. of words = 1.6464393031840794\n"
     ]
    }
   ],
   "source": [
    "clamp_entities = pred_df_clamp.drop_duplicates([\"Start\", \"End\", \"paper\"])[\"Entity\"]\n",
    "print(\"CLAMP mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in clamp_entities]))\n",
    "print(\"CLAMP std entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in clamp_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all CLAMP entities should have 1 CUI\n",
    "if FILTER:\n",
    "    cui_df = pred_df_temp.groupby([\"paper\", \"Start\", \"End\"])[\"CUI\"].count().reset_index()\n",
    "    cui_df = cui_df[cui_df[\"CUI\"]!=1].sort_values(by=\"CUI\", ascending=False)\n",
    "    assert len(cui_df.merge(pred_df_temp, on=[\"paper\", \"Start\", \"End\"])) == 0\n",
    "    assert len(pred_df_temp[(pred_df_temp[\"CUI\"].str.len()==8)&((pred_df_temp[\"TUI\"].isnull()))]) == 0 # all valid CUI should have a TUI after filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction C0524528 (pervasive developmental disorder): 0.0\n",
      "Fraction C1510586 (autism spectrum disorders): 0.0\n",
      "Fraction C0018817 (atrial septal defect): 1.0\n"
     ]
    }
   ],
   "source": [
    "# CUI for ASD/ASDs\n",
    "asd_cui = pred_df_clamp[(pred_df_clamp[\"Entity\"]==\"ASD\")|(pred_df_clamp[\"Entity\"]==\"ASDs\")]\n",
    "\n",
    "print(\"Fraction C0524528 (pervasive developmental disorder):\", len(asd_cui[asd_cui[\"CUI\"]==\"C0524528\"])/len(asd_cui))\n",
    "print(\"Fraction C1510586 (autism spectrum disorders):\", len(asd_cui[asd_cui[\"CUI\"]==\"C1510586\"])/len(asd_cui))\n",
    "print(\"Fraction C0018817 (atrial septal defect):\", len(asd_cui[asd_cui[\"CUI\"]==\"C0018817\"])/len(asd_cui))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C0018817    24583\n",
       "Name: CUI, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_cui[\"CUI\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test overlapping NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMP results\n",
      "Number of true positives = 96235\n",
      "Number of positive labels = 106284\n",
      "Number of positive predictions = 370654\n",
      "\n",
      "Precision = 0.2596356710031458\n",
      "Recall = 0.9054514320123442\n",
      "F-Measure = 0.40355350171301085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save CLAMP results to file\n",
    "if ABSTRACT:\n",
    "    CLAMP_file = filtered + \"clamp_statistics_abstract.txt\"\n",
    "else:\n",
    "    CLAMP_file = filtered + \"clamp_statistics_fulltext.txt\"\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(os.path.join(STATS_DIR, CLAMP_file), \"w\") as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.  \n",
    "    print(\"CLAMP results\")\n",
    "    clamp_true_pos_df, x, y = calculate_statistics(pred_df_clamp, labels_df, match_cui=False)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "    \n",
    "with open(os.path.join(STATS_DIR, CLAMP_file), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true positives, false positives, false negatives\n",
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(clamp_true_pos_df, pred_df_clamp, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "true_pos_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_true_positive.csv\"), index=False)\n",
    "clamp_true_pos_df.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_true_positive_all.csv\"), index=False)\n",
    "false_pos_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_positive.csv\"), index=False)\n",
    "false_neg_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_negative.csv\"), index=False)\n",
    "false_pos.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_positive_all.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018187858151316336"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fraction of general words\n",
    "len(clamp_true_pos_df[clamp_true_pos_df[\"TYPE\"]==\"General\"])/len(clamp_true_pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMP results\n",
      "Number of true positives = 1694\n",
      "Number of positive labels = 3129\n",
      "Number of positive predictions = 279637\n",
      "\n",
      "Precision = 0.006057853574455455\n",
      "Recall = 0.5413870246085011\n",
      "F-Measure = 0.011981638527970124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save CLAMP results to file\n",
    "if ABSTRACT:\n",
    "    CLAMP_file = filtered + \"clamp_statistics_abstract_general.txt\"\n",
    "else:\n",
    "    CLAMP_file = filtered + \"clamp_statistics_fulltext_general.txt\"\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(os.path.join(STATS_DIR, CLAMP_file), \"w\") as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.  \n",
    "    print(\"CLAMP results\")\n",
    "    clamp_true_pos_df, pred_df_clamp_general, labels_df_general = calculate_statistics(pred_df_clamp, labels_df, match_cui=False, match_general=True)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "    \n",
    "with open(os.path.join(STATS_DIR, CLAMP_file), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true positives, false positives, false negatives\n",
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(clamp_true_pos_df, pred_df_clamp_general, labels_df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "true_pos_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_true_positive_general.csv\"), index=False)\n",
    "false_pos_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_positive_general.csv\"), index=False)\n",
    "false_neg_grouped.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_negative_general.csv\"), index=False)\n",
    "false_pos.to_csv(os.path.join(CLAMP_RESULTS_DIRECTORY, filtered + \"clamp_false_positive_all_general.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cTAKES analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations that can be modified\n",
    "CTAKES_DIRECTORY = \"ctakes\" # parent directory for cTAKES-related files\n",
    "\n",
    "# formatted cTAKES output/predictions\n",
    "CTAKES_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_full_text\")\n",
    "CTAKES_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_abstract\")\n",
    "\n",
    "if ABSTRACT:\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_ABSTRACT \n",
    "else:\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pd.read_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, \"ctakes_preds.csv\"))\n",
    "\n",
    "# formatting\n",
    "pred_df_temp[\"Entity\"] = pred_df_temp[\"Entity_matched\"].str.strip()\n",
    "pred_df_temp[\"Entity_lower\"] = pred_df_temp[\"Entity\"].str.strip().str.lower()\n",
    "pred_df_temp = pred_df_temp.drop(['conditional', 'confidence', 'generic', 'id', 'negated', 'preferred_text', 'refsem', 'scheme', 'score', 'subject',\n",
    "       'textsem', 'uncertainty', 'true_text', 'part_of_speech', 'Entity_matched'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with extra predictions: 507\n"
     ]
    }
   ],
   "source": [
    "if not ABSTRACT:\n",
    "    assert (len((set(labels_df[\"paper\"])).difference(set(pred_df_temp[\"paper\"])))) == 0 # all papers were analyzed\n",
    "print(\"Number of papers with extra predictions:\", len((set(pred_df_temp[\"paper\"])).difference(set(labels_df[\"paper\"])))) # number of papers with extra predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:    \n",
    "    pred_df_temp = filter_pred(pred_df_temp, remove_non_asd=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropping duplicates: 938888\n",
      "after dropping duplicates: 511019\n"
     ]
    }
   ],
   "source": [
    "print(\"before dropping duplicates:\", len(pred_df_temp))\n",
    "pred_df_temp = pred_df_temp.drop_duplicates([\"Start\", \"End\", \"paper\", \"CUI\"])\n",
    "print(\"after dropping duplicates:\", len(pred_df_temp))\n",
    "pred_df_ctakes = pred_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cTAKES mean entity no. of words = 1.1616280412274298\n",
      "cTAKES std entity no. of words = 0.452216646287674\n"
     ]
    }
   ],
   "source": [
    "ctakes_entities = pred_df_ctakes[\"Entity\"]\n",
    "print(\"cTAKES mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in ctakes_entities]))\n",
    "print(\"cTAKES std entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in ctakes_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>CUI_x</th>\n",
       "      <th>CUI_y</th>\n",
       "      <th>TUI</th>\n",
       "      <th>Sentence_pred</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Entity_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28733900.txt</td>\n",
       "      <td>923</td>\n",
       "      <td>926</td>\n",
       "      <td>5</td>\n",
       "      <td>C0164707</td>\n",
       "      <td>T116</td>\n",
       "      <td>(Epi-)genetic factors which may increase susce...</td>\n",
       "      <td>Epi</td>\n",
       "      <td>epi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28733900.txt</td>\n",
       "      <td>923</td>\n",
       "      <td>926</td>\n",
       "      <td>5</td>\n",
       "      <td>C0014563</td>\n",
       "      <td>T109</td>\n",
       "      <td>(Epi-)genetic factors which may increase susce...</td>\n",
       "      <td>Epi</td>\n",
       "      <td>epi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28733900.txt</td>\n",
       "      <td>923</td>\n",
       "      <td>926</td>\n",
       "      <td>5</td>\n",
       "      <td>C0014582</td>\n",
       "      <td>T109</td>\n",
       "      <td>(Epi-)genetic factors which may increase susce...</td>\n",
       "      <td>Epi</td>\n",
       "      <td>epi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28733900.txt</td>\n",
       "      <td>923</td>\n",
       "      <td>926</td>\n",
       "      <td>5</td>\n",
       "      <td>C0267963</td>\n",
       "      <td>T047</td>\n",
       "      <td>(Epi-)genetic factors which may increase susce...</td>\n",
       "      <td>Epi</td>\n",
       "      <td>epi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28733900.txt</td>\n",
       "      <td>923</td>\n",
       "      <td>926</td>\n",
       "      <td>5</td>\n",
       "      <td>C0451152</td>\n",
       "      <td>T060</td>\n",
       "      <td>(Epi-)genetic factors which may increase susce...</td>\n",
       "      <td>Epi</td>\n",
       "      <td>epi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39062</th>\n",
       "      <td>22285033.txt</td>\n",
       "      <td>1101</td>\n",
       "      <td>1104</td>\n",
       "      <td>2</td>\n",
       "      <td>C0337380</td>\n",
       "      <td>T060</td>\n",
       "      <td>ERP responses to eye gaze may help characteriz...</td>\n",
       "      <td>ERP</td>\n",
       "      <td>erp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39063</th>\n",
       "      <td>22285821.txt</td>\n",
       "      <td>435</td>\n",
       "      <td>442</td>\n",
       "      <td>2</td>\n",
       "      <td>C0011923</td>\n",
       "      <td>T060</td>\n",
       "      <td>To accomplish this goal, we performed function...</td>\n",
       "      <td>imaging</td>\n",
       "      <td>imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39064</th>\n",
       "      <td>22285821.txt</td>\n",
       "      <td>435</td>\n",
       "      <td>442</td>\n",
       "      <td>2</td>\n",
       "      <td>C0079595</td>\n",
       "      <td>T060</td>\n",
       "      <td>To accomplish this goal, we performed function...</td>\n",
       "      <td>imaging</td>\n",
       "      <td>imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39065</th>\n",
       "      <td>9990834.txt</td>\n",
       "      <td>299</td>\n",
       "      <td>307</td>\n",
       "      <td>2</td>\n",
       "      <td>C1879338</td>\n",
       "      <td>T033</td>\n",
       "      <td>Dislocation is described as not just part of t...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>mourning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39066</th>\n",
       "      <td>9990834.txt</td>\n",
       "      <td>299</td>\n",
       "      <td>307</td>\n",
       "      <td>2</td>\n",
       "      <td>C0018235</td>\n",
       "      <td>T041</td>\n",
       "      <td>Dislocation is described as not just part of t...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>mourning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39067 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              paper  Start   End  CUI_x     CUI_y   TUI  \\\n",
       "0      28733900.txt    923   926      5  C0164707  T116   \n",
       "1      28733900.txt    923   926      5  C0014563  T109   \n",
       "2      28733900.txt    923   926      5  C0014582  T109   \n",
       "3      28733900.txt    923   926      5  C0267963  T047   \n",
       "4      28733900.txt    923   926      5  C0451152  T060   \n",
       "...             ...    ...   ...    ...       ...   ...   \n",
       "39062  22285033.txt   1101  1104      2  C0337380  T060   \n",
       "39063  22285821.txt    435   442      2  C0011923  T060   \n",
       "39064  22285821.txt    435   442      2  C0079595  T060   \n",
       "39065   9990834.txt    299   307      2  C1879338  T033   \n",
       "39066   9990834.txt    299   307      2  C0018235  T041   \n",
       "\n",
       "                                           Sentence_pred    Entity  \\\n",
       "0      (Epi-)genetic factors which may increase susce...       Epi   \n",
       "1      (Epi-)genetic factors which may increase susce...       Epi   \n",
       "2      (Epi-)genetic factors which may increase susce...       Epi   \n",
       "3      (Epi-)genetic factors which may increase susce...       Epi   \n",
       "4      (Epi-)genetic factors which may increase susce...       Epi   \n",
       "...                                                  ...       ...   \n",
       "39062  ERP responses to eye gaze may help characteriz...       ERP   \n",
       "39063  To accomplish this goal, we performed function...   imaging   \n",
       "39064  To accomplish this goal, we performed function...   imaging   \n",
       "39065  Dislocation is described as not just part of t...  mourning   \n",
       "39066  Dislocation is described as not just part of t...  mourning   \n",
       "\n",
       "      Entity_lower  \n",
       "0              epi  \n",
       "1              epi  \n",
       "2              epi  \n",
       "3              epi  \n",
       "4              epi  \n",
       "...            ...  \n",
       "39062          erp  \n",
       "39063      imaging  \n",
       "39064      imaging  \n",
       "39065     mourning  \n",
       "39066     mourning  \n",
       "\n",
       "[39067 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if any entities have more than 1 CUI\n",
    "cui_df = pred_df_temp.groupby([\"paper\", \"Start\", \"End\"])[\"CUI\"].count().reset_index()\n",
    "cui_df = cui_df[cui_df[\"CUI\"]!=1].sort_values(by=\"CUI\", ascending=False)\n",
    "cui_df.merge(pred_df_temp, on=[\"paper\", \"Start\", \"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction C0524528 (pervasive developmental disorder): 0.0\n",
      "Fraction C1510586 (autism spectrum disorders): 0.0\n",
      "Fraction C0018817 (atrial septal defect): 1.0\n"
     ]
    }
   ],
   "source": [
    "# CUI for ASD/ASDs\n",
    "asd_cui = pred_df_ctakes[(pred_df_ctakes[\"Entity\"]==\"ASD\")|(pred_df_ctakes[\"Entity\"]==\"ASDs\")]\n",
    "\n",
    "print(\"Fraction C0524528 (pervasive developmental disorder):\", len(asd_cui[asd_cui[\"CUI\"]==\"C0524528\"])/len(asd_cui))\n",
    "print(\"Fraction C1510586 (autism spectrum disorders):\", len(asd_cui[asd_cui[\"CUI\"]==\"C1510586\"])/len(asd_cui))\n",
    "print(\"Fraction C0018817 (atrial septal defect):\", len(asd_cui[asd_cui[\"CUI\"]==\"C0018817\"])/len(asd_cui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C0018817    37759\n",
       "Name: CUI, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_cui[\"CUI\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test overlapping NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cTAKES results\n",
      "Number of true positives = 101219\n",
      "Number of positive labels = 106284\n",
      "Number of positive predictions = 489520\n",
      "\n",
      "Precision = 0.20677193985945416\n",
      "Recall = 0.9523446614730345\n",
      "F-Measure = 0.33977281119294267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save cTAKES results to file\n",
    "if ABSTRACT:\n",
    "    CTAKES_file = filtered + \"ctakes_statistics_abstract.txt\"\n",
    "else:\n",
    "    CTAKES_file = filtered + \"ctakes_statistics_fulltext.txt\"\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(os.path.join(STATS_DIR, CTAKES_file), \"w\") as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.  \n",
    "    print(\"cTAKES results\")\n",
    "    ctakes_true_pos_df, x, y = calculate_statistics(pred_df_ctakes, labels_df, match_cui=False)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "    \n",
    "with open(os.path.join(STATS_DIR, CTAKES_file), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true positives, false positives, false negatives\n",
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(ctakes_true_pos_df, pred_df_ctakes, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "true_pos_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_true_positive.csv\"), index=False)\n",
    "ctakes_true_pos_df.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_true_positive_all.csv\"), index=False)\n",
    "false_pos_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_positive.csv\"), index=False)\n",
    "false_neg_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_negative.csv\"), index=False)\n",
    "false_pos.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_positive_all.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007776099999192513"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fraction of general words\n",
    "len(ctakes_true_pos_df[ctakes_true_pos_df[\"TYPE\"]==\"General\"])/len(ctakes_true_pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cTAKES results\n",
      "Number of true positives = 641\n",
      "Number of positive labels = 3129\n",
      "Number of positive predictions = 366659\n",
      "\n",
      "Precision = 0.001748218371838684\n",
      "Recall = 0.2048577820389901\n",
      "F-Measure = 0.0034668512769478726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save cTAKES results to file\n",
    "if ABSTRACT:\n",
    "    CTAKES_file = filtered + \"ctakes_statistics_abstract_general.txt\"\n",
    "else:\n",
    "    CTAKES_file = filtered + \"ctakes_statistics_fulltext_general.txt\"\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(os.path.join(STATS_DIR, CTAKES_file), \"w\") as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.  \n",
    "    print(\"cTAKES results\")\n",
    "    ctakes_true_pos_df, pred_df_ctakes_general, labels_df_general = calculate_statistics(pred_df_ctakes, labels_df, match_cui=False, match_general=True)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "    \n",
    "with open(os.path.join(STATS_DIR, CTAKES_file), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true positives, false positives, false negatives\n",
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(ctakes_true_pos_df, pred_df_ctakes_general, labels_df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "true_pos_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_true_positive_general.csv\"), index=False)\n",
    "false_pos_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_positive_general.csv\"), index=False)\n",
    "false_neg_grouped.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_negative_general.csv\"), index=False)\n",
    "false_pos.to_csv(os.path.join(CTAKES_RESULTS_DIRECTORY, filtered + \"ctakes_false_positive_all_general.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaMap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations that can be modified\n",
    "METAMAP_DIRECTORY = \"metamap\"\n",
    "\n",
    "# formatted MetaMap output/predictions\n",
    "METAMAP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_full_text\")\n",
    "METAMAP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_abstract\")\n",
    "\n",
    "if ABSTRACT:\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_ABSTRACT \n",
    "else:\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_original_df.copy()\n",
    "labels_df_temp = pd.read_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in BM ASD terms and add TUI\n",
    "BM_df = pd.read_csv(\"BM_terms.csv\")\n",
    "BM_df.rename(columns={\"CUI\": \"CUI_original\"}, inplace=True)\n",
    "BM_df[\"NEGATED\"] = BM_df[\"CUI_original\"].apply(lambda x: str(x)[0] == \"-\")\n",
    "BM_df[\"CUI\"] = BM_df[\"CUI_original\"].apply(lambda x: str(x).replace(\"-\", \"\"))\n",
    "BM_cui_to_tui_df = pd.read_csv(\"tui_list_BM.txt\", sep=\"\\t\", index_col=0, header=None).reset_index()\n",
    "BM_cui_to_tui_df.columns = [\"CUI\", \"TUI\"]\n",
    "BM_df = BM_df.merge(BM_cui_to_tui_df, how=\"left\")\n",
    "BM_df[\"TEXT\"] = BM_df[\"TEXT\"].str.strip().str.lower()\n",
    "BM_df = BM_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge labels with BM term information\n",
    "labels_df_temp = labels_df_temp.merge(BM_df, left_on=\"Entity_lower\", right_on=\"TEXT\", how=\"left\")\n",
    "\n",
    "# clean-up\n",
    "labels_df_temp = labels_df_temp.replace({'Entity_lower': {\"asperger 's\": \"asperger's\"}})\n",
    "labels_df_temp = labels_df_temp.replace({'Entity': {\"asperger 's\": \"asperger's\"}})\n",
    "labels_df_temp = labels_df_temp.replace({'Entity': {\"Asperger 's\": \"Asperger's\"}})\n",
    "\n",
    "# case-sensitive for ASD and ASDs\n",
    "labels_df_temp = labels_df_temp[~((labels_df_temp[\"Entity_lower\"]==\"asds\")&(labels_df_temp[\"Entity\"]!=\"ASDs\"))]\n",
    "labels_df_temp = labels_df_temp[~((labels_df_temp[\"Entity_lower\"]==\"asd\")&(labels_df_temp[\"Entity\"]!=\"ASD\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_temp = pd.read_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_preds.csv\"))\n",
    "\n",
    "# formatting\n",
    "pred_df_temp = pred_df_temp[~((pred_df_temp[\"Entity_matched\"]==\"Body\")&(pred_df_temp[\"Start\"]==5))] # filter out Body separator in text\n",
    "pred_df_temp[\"Entity_original\"] = pred_df_temp[\"Entity\"]\n",
    "pred_df_temp[\"Entity\"] = pred_df_temp[\"Entity_matched\"].str.strip()\n",
    "pred_df_temp[\"Entity_lower\"] = pred_df_temp[\"Entity\"].str.strip().str.lower()\n",
    "\n",
    "# add TUI to predictions\n",
    "semantic_types_df = pd.read_csv(\"SemanticTypes_2018AB.txt\", sep=\"|\", header=None)\n",
    "semantic_types_df.columns = [\"SemType\", \"TUI\", \"SemType_long\"]\n",
    "pred_df_temp = pred_df_temp.merge(semantic_types_df, how=\"left\", on=\"SemType\")\n",
    "\n",
    "# clean up unused columns\n",
    "pred_df_temp = pred_df_temp.drop(['MappingScore', 'Negated',\n",
    "       'CandidateScore', 'MatchedWords', 'MatchedPhrase',\n",
    "       'paper_part', 'Entity_matched',\n",
    "       'Entity_original', 'SemType', 'SemType_long'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ABSTRACT:\n",
    "    assert (len((set(labels_df[\"paper\"])).difference(set(pred_df_temp[\"paper\"])))) == 0 # all papers were analyzed\n",
    "    assert (len((set(labels_df_temp[\"paper\"])).difference(set(pred_df_temp[\"paper\"])))) == 0 # all papers were analyzed\n",
    "\n",
    "print(\"Number of papers with extra predictions:\", len((set(pred_df_temp[\"paper\"])).difference(set(labels_df[\"paper\"])))) # number of papers with extra predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER:\n",
    "    pred_df_temp = filter_pred(pred_df_temp, remove_non_asd=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any entities have more than 1 CUI\n",
    "cui_df = pred_df_temp.groupby([\"paper\", \"Start\", \"End\"])[\"CUI\"].count().reset_index()\n",
    "cui_df = cui_df[cui_df[\"CUI\"]!=1].sort_values(by=\"CUI\", ascending=False)\n",
    "cui_df.merge(pred_df_temp, on=[\"paper\", \"Start\", \"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df_temp\n",
    "pred_df_metamap = pred_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamap_entities = pred_df_metamap.drop_duplicates([\"Start\", \"End\", \"paper\"])[\"Entity\"]\n",
    "print(\"METAMAP mean entity no. of words =\", np.mean([len(str(ent).split(\" \")) for ent in metamap_entities]))\n",
    "print(\"METAMAP std entity no. of words =\", np.std([len(str(ent).split(\" \")) for ent in metamap_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUI for ASD/ASDs\n",
    "asd_cui = pred_df_metamap[(pred_df_metamap[\"Entity\"]==\"ASD\")|(pred_df_metamap[\"Entity\"]==\"ASDs\")]\n",
    "print(\"Fraction C0524528 (pervasive developmental disorder):\", len(asd_cui[asd_cui[\"CUI\"]==\"C0524528\"])/len(asd_cui))\n",
    "print(\"Fraction C1510586 (autism spectrum disorders):\", len(asd_cui[asd_cui[\"CUI\"]==\"C1510586\"])/len(asd_cui))\n",
    "print(\"Fraction C0018817 (atrial septal defect):\", len(asd_cui[asd_cui[\"CUI\"]==\"C0018817\"])/len(asd_cui))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asd_cui[\"CUI\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test overlapping NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_df_metamap)\n",
    "pred_df_metamap = pred_df_metamap.drop_duplicates([\"paper\", \"Start\", \"End\"])\n",
    "len(pred_df_metamap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cTAKES results to file\n",
    "if ABSTRACT:\n",
    "    METAMAP_file = filtered + \"metamap_statistics_abstract.txt\"\n",
    "else:\n",
    "    METAMAP_file = filtered + \"metamap_statistics_fulltext.txt\"\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(os.path.join(STATS_DIR, METAMAP_file), \"w\") as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.  \n",
    "    print(\"MetaMap results\")\n",
    "    metamap_true_pos_df, x, y = calculate_statistics(pred_df_metamap, labels_df, match_cui=False)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "    \n",
    "with open(os.path.join(STATS_DIR, METAMAP_file), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true positives, false positives, false negatives\n",
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(metamap_true_pos_df, pred_df_metamap, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "true_pos_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_true_positive.csv\"), index=False)\n",
    "metamap_true_pos_df.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_true_positive_all.csv\"), index=False)\n",
    "false_pos_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_positive.csv\"), index=False)\n",
    "false_neg_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_negative.csv\"), index=False)\n",
    "false_pos.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_positive_all.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of general words\n",
    "len(metamap_true_pos_df[metamap_true_pos_df[\"TYPE\"]==\"General\"])/len(metamap_true_pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cTAKES results to file\n",
    "if ABSTRACT:\n",
    "    METAMAP_file = filtered + \"metamap_statistics_abstract_general.txt\"\n",
    "else:\n",
    "    METAMAP_file = filtered + \"metamap_statistics_fulltext_general.txt\"\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(os.path.join(STATS_DIR, METAMAP_file), \"w\") as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.  \n",
    "    print(\"MetaMap results\")\n",
    "    metamap_true_pos_df, pred_df_metamap_general, labels_df_general = calculate_statistics(pred_df_metamap, labels_df, match_cui=False, match_general=True)\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "    \n",
    "with open(os.path.join(STATS_DIR, METAMAP_file), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true positives, false positives, false negatives\n",
    "true_pos_grouped, false_pos_grouped, false_neg_grouped, false_pos, false_neg = get_false_and_true_pos(metamap_true_pos_df, pred_df_metamap_general, labels_df_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "true_pos_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_true_positive_general.csv\"), index=False)\n",
    "false_pos_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_positive_general.csv\"), index=False)\n",
    "false_neg_grouped.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_negative_general.csv\"), index=False)\n",
    "false_pos.to_csv(os.path.join(METAMAP_RESULTS_DIRECTORY, filtered + \"metamap_false_positive_all_general.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
