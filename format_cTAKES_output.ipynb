{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook formats the cTAKES output (i.e. predictions) for the PubMed abstracts and full-texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations that can be modified\n",
    "ABSTRACT = False # True if running this program of abstracts, False if running on full-texts\n",
    "CTAKES_DIRECTORY = \"ctakes\" # parent directory for cTAKES-related files\n",
    "\n",
    "# location of full-texts/abstracts in plain text\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_fulltexts_544\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_20408\"\n",
    "\n",
    "# cTAKES output/predictions\n",
    "CTAKES_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_output_full_text\") \n",
    "CTAKES_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_output_abstract\") \n",
    "\n",
    "# formatted cTAKES output/predictions\n",
    "CTAKES_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_full_text\")\n",
    "CTAKES_RESULTS_DIRECTORY_ABSTRACT = os.path.join(CTAKES_DIRECTORY, \"ctakes_results_abstract\")\n",
    "\n",
    "if ABSTRACT:\n",
    "    CTAKES_OUTPUT_DIRECTORY = CTAKES_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "else:\n",
    "    CTAKES_OUTPUT_DIRECTORY = CTAKES_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    CTAKES_RESULTS_DIRECTORY = CTAKES_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def extract_sentence(sentences, row):\n",
    "    ent_start = row['Start']\n",
    "    for s in sentences:\n",
    "        if s.start_char <= ent_start and ent_start < s.end_char:\n",
    "            return s.text\n",
    "    return \"\"\n",
    "\n",
    "def is_file_empty(DIRECTORY, filename):\n",
    "    with open(os.path.join(DIRECTORY, filename)) as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    return data.isspace() or data == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PMC5681971.csv\n",
      "5 PMC5751211.csv\n",
      "10 PMC4437371.csv\n",
      "15 PMC6509633.csv\n",
      "20 PMC5894192.csv\n",
      "25 PMC5414938.csv\n",
      "30 PMC5570931.csv\n",
      "35 PMC5516334.csv\n",
      "40 PMC3134991.csv\n",
      "45 PMC6099375.csv\n",
      "50 PMC6394789.csv\n",
      "55 PMC6445349.csv\n",
      "60 PMC6263710.csv\n",
      "65 PMC5967343.csv\n",
      "70 PMC5879814.csv\n",
      "75 PMC4938426.csv\n",
      "80 PMC5800535.csv\n",
      "85 PMC5577136.csv\n",
      "90 PMC4973977.csv\n",
      "95 PMC3183050.csv\n",
      "100 PMC3150222.csv\n",
      "105 PMC6373074.csv\n",
      "110 PMC6370186.csv\n",
      "115 PMC5264460.csv\n",
      "120 PMC6153902.csv\n",
      "125 PMC6308558.csv\n",
      "130 PMC5443871.csv\n",
      "135 PMC6245048.csv\n",
      "140 PMC6022232.csv\n",
      "145 PMC6341103.csv\n",
      "150 PMC6050263.csv\n",
      "155 PMC5360852.csv\n",
      "160 PMC5225659.csv\n",
      "165 PMC6606653.csv\n",
      "170 PMC6223803.csv\n",
      "175 PMC5947578.csv\n",
      "180 PMC5697632.csv\n",
      "185 PMC6017261.csv\n",
      "190 PMC6539237.csv\n",
      "195 PMC5526322.csv\n",
      "200 PMC6550041.csv\n",
      "205 PMC5678184.csv\n",
      "210 PMC6181799.csv\n",
      "215 PMC5457440.csv\n",
      "220 PMC5832686.csv\n",
      "225 PMC6258483.csv\n",
      "230 PMC6651320.csv\n",
      "235 PMC3411605.csv\n",
      "240 PMC5241332.csv\n",
      "245 PMC6085908.csv\n",
      "250 PMC5623420.csv\n",
      "255 PMC5317002.csv\n",
      "260 PMC5455122.csv\n",
      "265 PMC6813284.csv\n",
      "270 PMC5897402.csv\n",
      "275 PMC5537407.csv\n",
      "280 PMC5948870.csv\n",
      "285 PMC4506065.csv\n",
      "290 PMC4701177.csv\n",
      "295 PMC5286449.csv\n",
      "300 PMC4719705.csv\n",
      "305 PMC4689504.csv\n",
      "310 PMC5942347.csv\n",
      "315 PMC6292033.csv\n",
      "320 PMC4801394.csv\n",
      "325 PMC5584984.csv\n",
      "330 PMC5943853.csv\n",
      "335 PMC5119728.csv\n",
      "340 PMC6707962.csv\n",
      "345 PMC5299396.csv\n",
      "350 PMC5837293.csv\n",
      "355 PMC6282609.csv\n",
      "360 PMC6675248.csv\n",
      "365 PMC5625847.csv\n",
      "370 PMC6199253.csv\n",
      "375 PMC5678163.csv\n",
      "380 PMC5718481.csv\n",
      "385 PMC4556482.csv\n",
      "390 PMC5701449.csv\n",
      "395 PMC5789211.csv\n",
      "400 PMC4529109.csv\n",
      "405 PMC2958801.csv\n",
      "410 PMC6477793.csv\n",
      "415 PMC5198096.csv\n",
      "420 PMC5681818.csv\n",
      "425 PMC4577211.csv\n",
      "430 PMC5636137.csv\n",
      "435 PMC6693568.csv\n",
      "440 PMC6158250.csv\n",
      "445 PMC6521002.csv\n",
      "450 PMC6122267.csv\n",
      "455 PMC6158468.csv\n",
      "460 PMC5370427.csv\n",
      "465 PMC5318566.csv\n",
      "470 PMC3827355.csv\n",
      "475 PMC6450836.csv\n",
      "480 PMC6122716.csv\n",
      "485 PMC5357301.csv\n",
      "490 PMC3352915.csv\n",
      "495 PMC5504648.csv\n",
      "500 PMC5813031.csv\n",
      "505 PMC3103507.csv\n",
      "510 PMC6680328.csv\n",
      "515 PMC4638355.csv\n",
      "520 PMC6628259.csv\n",
      "525 PMC5372633.csv\n",
      "530 PMC5015758.csv\n",
      "535 PMC5905863.csv\n",
      "540 PMC5192959.csv\n"
     ]
    }
   ],
   "source": [
    "# format cTAKES output/predictions in csv format where one row is one NER prediction\n",
    "empty_input_files = []\n",
    "with open(os.path.join(CTAKES_RESULTS_DIRECTORY, \"ctakes_preds.csv\"), \"w\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    header = ['conditional', 'confidence', 'CUI', 'generic', 'id', 'negated', 'End',\n",
    "       'Start', 'preferred_text', 'refsem', 'scheme', 'score', 'subject',\n",
    "       'textsem', 'TUI', 'uncertainty', 'true_text', 'part_of_speech', 'paper',\n",
    "       'Entity_matched', 'Sentence_pred']\n",
    "    csv_writer.writerow(header)\n",
    "    \n",
    "    ctakes_files = [filename for filename in os.listdir(CTAKES_OUTPUT_DIRECTORY) if filename.endswith(\".csv\")]\n",
    "    for idx, filename in enumerate(ctakes_files):\n",
    "\n",
    "        if ABSTRACT and idx % 1000 == 0:\n",
    "            print(idx, filename)\n",
    "        elif not ABSTRACT and idx % 5 == 0:\n",
    "            print(idx, filename)\n",
    "\n",
    "        input_filename = filename.replace(\".csv\", \".txt\")\n",
    "            \n",
    "        # ignore empty files\n",
    "        if is_file_empty(INPUT_DIRECTORY, input_filename):\n",
    "            empty_input_files.append(input_filename)\n",
    "            continue\n",
    "        \n",
    "        if is_file_empty(CTAKES_OUTPUT_DIRECTORY, filename):\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(INPUT_DIRECTORY, input_filename)) as f:\n",
    "            plain_text = f.read()     \n",
    "            doc = nlp(plain_text)\n",
    "            \n",
    "        df = pd.read_csv(os.path.join(CTAKES_OUTPUT_DIRECTORY, filename))\n",
    "        df[\"paper\"] = filename.replace(\".csv\", \".txt\")\n",
    "        df = df.rename(columns={\"cui\":\"CUI\", \"tui\":\"TUI\", \"pos_start\":\"Start\", \"pos_end\":\"End\"})\n",
    "        df[\"Entity_matched\"] = df.apply(lambda row: plain_text[row['Start']:row['End']], axis=1)\n",
    "        df[\"Sentence_pred\"] = df.apply(lambda row: extract_sentence(doc.sents, row), axis=1)\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            csv_writer.writerow(list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(empty_input_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
