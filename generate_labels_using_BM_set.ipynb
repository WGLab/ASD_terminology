{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook labels the PubMed full-texts and abstracts using the benchmark (BM) ASD vocabulary set. The labels/matches are exported as a .csv file. These labels are considered the \"true\" labels for the named-entity recognition (NER) task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from unidecode import unidecode\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations that can be modified\n",
    "ABSTRACT = True # True if running this program of abstracts, False if running on full-texts\n",
    "OUTPUT_DIR = \"BM_labelled\" # folder where the results (dataframe with labels) will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ABSTRACT:\n",
    "    INPUT_DIRECTORY = \"pubmed_abstracts_20408\"\n",
    "else:\n",
    "    INPUT_DIRECTORY = \"pubmed_fulltexts_544\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 827 autism terms\n"
     ]
    }
   ],
   "source": [
    "# read in BM ASD terms and create BM set (all lowercase)\n",
    "BM_df = pd.read_csv(\"BM_terms.csv\")\n",
    "BM_df[\"TEXT\"] = BM_df[\"TEXT\"].str.strip().str.lower()\n",
    "autism_terms = set(BM_df[\"TEXT\"])\n",
    "print(f\"There are {len(autism_terms)} autism terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaCy Phrase Matcher (used for labelling BM terms)\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(text) for text in autism_terms]\n",
    "matcher.add(\"AutismTerms\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file to export labels to\n",
    "if ABSTRACT:\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"abstract_labels.csv\")\n",
    "else:\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"full_text_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28770039.txt\n",
      "1000 24761747.txt\n",
      "2000 29873809.txt\n",
      "3000 18760197.txt\n",
      "4000 17517680.txt\n",
      "5000 28856484.txt\n",
      "6000 18563708.txt\n",
      "7000 16389586.txt\n",
      "8000 29927797.txt\n",
      "9000 18702558.txt\n",
      "10000 23535821.txt\n",
      "11000 25882392.txt\n",
      "12000 8050988.txt\n",
      "13000 29664902.txt\n",
      "14000 7169196.txt\n",
      "15000 25939529.txt\n",
      "16000 19589455.txt\n",
      "17000 24604922.txt\n",
      "18000 24167375.txt\n",
      "19000 23159942.txt\n",
      "20000 17896119.txt\n"
     ]
    }
   ],
   "source": [
    "# label BM terms and write the results to the csv file where one row is a label/match\n",
    "\n",
    "with open(csv_path, \"w\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"Entity\", \"Entity_lower\", \"paper\", \"Start\", \"End\", \"Sentence\"]) # header\n",
    "    for idx, filename in enumerate(os.listdir(INPUT_DIRECTORY)):\n",
    "\n",
    "        if filename.endswith(\".txt\"):\n",
    "            path = os.path.join(INPUT_DIRECTORY, filename)\n",
    "\n",
    "            if ABSTRACT and idx % 1000 == 0:\n",
    "                print(idx, filename)\n",
    "            elif not ABSTRACT and idx % 5 == 0:\n",
    "                print(idx, filename)\n",
    "\n",
    "            # tag entities in abstract\n",
    "            with open(path, \"r\") as f:\n",
    "                data = f.read()\n",
    "            \n",
    "            # convert to ASCII\n",
    "            #data = unidecode(data)\n",
    "\n",
    "            doc = nlp(data)\n",
    "            matches = matcher(doc)\n",
    "            spans = []\n",
    "\n",
    "            for match_id, start, end in matches:\n",
    "                span = doc[start:end]\n",
    "                spans.append(span)\n",
    "\n",
    "            # use longest BM term match\n",
    "            filtered = spacy.util.filter_spans(spans)\n",
    "\n",
    "            for span in filtered:\n",
    "                row = [span.text, span.text.lower().strip(), filename, span.start_char, span.end_char, span.sent.text]\n",
    "                csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
