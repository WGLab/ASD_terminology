{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from unidecode import unidecode\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations that can be modified\n",
    "ABSTRACT = True # True if running this program of abstracts, False if running on full-texts\n",
    "METAMAP_DIRECTORY = \"metamap\"\n",
    "\n",
    "# location of full-texts/abstracts in plain text\n",
    "INPUT_DIRECTORY_FULL_TEXT = \"pubmed_fulltexts_544\"\n",
    "INPUT_DIRECTORY_ABSTRACT = \"pubmed_abstracts_20408\"\n",
    "\n",
    "# MetaMap output/predictions\n",
    "METAMAP_OUTPUT_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_output_fulltexts\") \n",
    "METAMAP_OUTPUT_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_output_abstracts\") \n",
    "\n",
    "# formatted MetaMap output/predictions\n",
    "METAMAP_RESULTS_DIRECTORY_FULL_TEXT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_full_text\")\n",
    "METAMAP_RESULTS_DIRECTORY_ABSTRACT = os.path.join(METAMAP_DIRECTORY, \"metamap_results_abstract\")\n",
    "\n",
    "if ABSTRACT:\n",
    "    METAMAP_OUTPUT_DIRECTORY = METAMAP_OUTPUT_DIRECTORY_ABSTRACT\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_ABSTRACT \n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_ABSTRACT\n",
    "    METAMAP_FULL_TEXT_DIRECTORY = os.path.join(METAMAP_DIRECTORY, \"metamap_abstract\") # combined text directory since MetaMap splits up text if too long\n",
    "else:\n",
    "    METAMAP_OUTPUT_DIRECTORY = METAMAP_OUTPUT_DIRECTORY_FULL_TEXT\n",
    "    METAMAP_RESULTS_DIRECTORY = METAMAP_RESULTS_DIRECTORY_FULL_TEXT\n",
    "    INPUT_DIRECTORY = INPUT_DIRECTORY_FULL_TEXT\n",
    "    METAMAP_FULL_TEXT_DIRECTORY = os.path.join(METAMAP_DIRECTORY, \"metamap_full_text\") # combined text directory since MetaMap splits up text if too long\n",
    "    METAMAP_TABLES_DIR = os.path.join(METAMAP_DIRECTORY, \"metamap_tables\") # tables in fulltexts that couldn't be processed by metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def extract_sentence(sentences, row):\n",
    "    ent_start = row['Start']\n",
    "    for s in sentences:\n",
    "        if s.start_char <= ent_start and ent_start < s.end_char:\n",
    "            return s.text\n",
    "    return \"\"\n",
    "\n",
    "def is_file_empty(DIRECTORY, filename):\n",
    "    with open(os.path.join(DIRECTORY, filename)) as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    return data.isspace() or data == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 827 autism terms\n"
     ]
    }
   ],
   "source": [
    "# read in BM ASD terms and create BM set (all lowercase)\n",
    "BM_df = pd.read_csv(\"BM_terms.csv\")\n",
    "BM_df[\"TEXT\"] = BM_df[\"TEXT\"].str.strip().str.lower()\n",
    "autism_terms = set(BM_df[\"TEXT\"])\n",
    "print(f\"There are {len(autism_terms)} autism terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaCy Phrase Matcher (used for labelling BM terms)\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(text) for text in autism_terms]\n",
    "matcher.add(\"AutismTerms\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange files so they are processed in order (MetaMap splits up text if too long)\n",
    "\n",
    "metamap_files = os.listdir(METAMAP_OUTPUT_DIRECTORY)\n",
    "metamap_files = [f for f in metamap_files if \".txt\" in f]\n",
    "if ABSTRACT:\n",
    "    metamap_files = sorted(metamap_files, key = lambda x: (x.split(\"_\")[0], int(x.split(\"_\")[1])))\n",
    "else:\n",
    "    metamap_files = sorted(metamap_files, key = lambda x: (x.split(\"_\")[0], int(x.split(\"_\")[1]), int(x.split(\"_\")[2])))\n",
    "    metamap_tables = os.listdir(METAMAP_TABLES_DIR)\n",
    "    metamap_tables = [f for f in metamap_tables if \".txt\" in f]\n",
    "    metamap_tables = sorted(metamap_tables, key = lambda x: (x.split(\"_\")[0], int(x.split(\"_\")[1]),))\n",
    "\n",
    "input_files = os.listdir(INPUT_DIRECTORY)\n",
    "input_files = [f for f in input_files if \".txt\" in f]\n",
    "input_files = [f.replace(\".txt\", \"\") for f in input_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10026453.txt_0\n",
      "1000 12352267.txt_0\n",
      "2000 15590241.txt_0\n",
      "3000 16835068.txt_0\n",
      "4000 17825125.txt_0\n",
      "5000 18998843.txt_0\n",
      "6000 2004485.txt_0\n",
      "7000 21610188.txt_0\n",
      "8000 22987894.txt_0\n",
      "9000 24384067.txt_0\n",
      "10000 25621974.txt_0\n",
      "11000 26273832.txt_0\n",
      "12000 26856821.txt_0\n",
      "13000 27483248.txt_0\n",
      "14000 28189493.txt_0\n",
      "15000 28707805.txt_0\n",
      "16000 29266810.txt_0\n",
      "17000 29885454.txt_0\n",
      "18000 30581125.txt_0\n",
      "19000 3666327.txt_0\n",
      "20000 8894948.txt_0\n"
     ]
    }
   ],
   "source": [
    "# format cTAKES output/predictions in csv format where one row is one NER prediction\n",
    "\n",
    "# output files\n",
    "labels_file = open(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_labels.csv\"), \"w\")\n",
    "preds_file = open(os.path.join(METAMAP_RESULTS_DIRECTORY, \"metamap_preds.csv\"), \"w\")\n",
    "\n",
    "labels_csv_writer = csv.writer(labels_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "labels_csv_writer.writerow([\"Entity\", \"Entity_lower\", \"paper\", \"Start\", \"End\", \"Sentence\"])\n",
    "\n",
    "preds_csv_writer = csv.writer(preds_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "metamap_columns = [\"id\", \"MappingScore\", \"CandidateCUI\", \"CandidateMatched\", \"SemType\", \"StartPos\", \"Length\", \"Negated\", \"CandidateScore\", \"MatchedWords\"]\n",
    "metamap_columns_formatted = ['MappingScore', 'CUI', 'Entity', 'SemType', 'Start', 'Negated',\n",
    "       'CandidateScore', 'MatchedWords', 'MatchedPhrase', 'paper',\n",
    "       'paper_part', 'End', 'Entity_matched', \"Sentence_pred\"]\n",
    "preds_csv_writer.writerow(metamap_columns_formatted)\n",
    "\n",
    "# metamap formatting\n",
    "header = \"id\tMappingScore\tCandidateCUI\tCandidateMatched\tSemType\tStartPos\tLength\tNegated\tCandidateScore\tMatchedWords\\n\"\n",
    "papers_analyzed = []\n",
    "full_text = \"\"\n",
    "empty_metamap_output = []\n",
    "for filename in metamap_files:\n",
    "    paper = filename.split(\"_\")[0]\n",
    "\n",
    "    # new paper\n",
    "    if paper not in papers_analyzed:\n",
    "\n",
    "        # label last paper for BM terms\n",
    "        if len(papers_analyzed) > 0:\n",
    "\n",
    "            full_text = unidecode(full_text)\n",
    "\n",
    "            with open(os.path.join(METAMAP_FULL_TEXT_DIRECTORY, papers_analyzed[-1]), \"w\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            # analyze previous paper and label BM terms with spaCy\n",
    "            doc = nlp(full_text)\n",
    "            matches = matcher(doc)\n",
    "            spans = []\n",
    "\n",
    "            for match_id, start, end in matches:\n",
    "                span = doc[start:end]\n",
    "                spans.append(span)\n",
    "\n",
    "            filtered = spacy.util.filter_spans(spans) # use longest match\n",
    "\n",
    "            for span in filtered:\n",
    "                row = [span.text, span.text.lower().strip(), papers_analyzed[-1], span.start_char, span.end_char, span.sent.text]\n",
    "                labels_csv_writer.writerow(row)\n",
    "\n",
    "        idx = len(papers_analyzed)\n",
    "        if ABSTRACT and idx % 1000 == 0:\n",
    "            print(idx, filename)\n",
    "        elif not ABSTRACT and idx % 5 == 0:\n",
    "            print(idx, filename)\n",
    "\n",
    "        full_text = \"\"\n",
    "        papers_analyzed.append(paper)\n",
    "\n",
    "\n",
    "    if is_file_empty(METAMAP_OUTPUT_DIRECTORY, filename): # ignore empty file\n",
    "        empty_metamap_output.append(filename)\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(METAMAP_OUTPUT_DIRECTORY, filename), \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    if header not in data:\n",
    "        print(filename, \"has no header\")\n",
    "\n",
    "    splits = data.split(header)\n",
    "\n",
    "    # this part contains the pmid and utterances\n",
    "    info = splits[0].split(\"\\n\")\n",
    "    pmid = \"\"\n",
    "    utterance = False\n",
    "    start_idx = len(full_text)\n",
    "\n",
    "    for line in info:\n",
    "        if \"PMID: \" in line:\n",
    "            pmid_found = line.replace(\"PMID: \", \"\")\n",
    "            pmid_found = pmid_found.split(\"_\")[0]\n",
    "\n",
    "            # check if pmid matches paper\n",
    "            if pmid_found != paper:\n",
    "                raise Exception(\"PMID doesn't match paper:\", line)\n",
    "            else:\n",
    "                pmid = pmid_found\n",
    "\n",
    "        if utterance:\n",
    "            full_text = full_text + line\n",
    "\n",
    "        if \"UttText:\" in line:\n",
    "            utterance = True\n",
    "        else:\n",
    "            utterance = False\n",
    "\n",
    "    full_text = full_text + \" \"\n",
    "\n",
    "    # no terms detected\n",
    "    if len(splits) < 2:\n",
    "        i = i + 1\n",
    "        continue\n",
    "\n",
    "    doc = nlp(full_text)\n",
    "\n",
    "    temp = pd.read_csv(StringIO(splits[1]), sep=\"\\t\", header=None) \n",
    "    temp.columns = metamap_columns\n",
    "    temp[\"MatchedPhrase\"] = temp[\"MatchedWords\"].apply(lambda x: \" \".join(str(x).split(\",\")))\n",
    "    temp[\"paper\"] = paper\n",
    "    temp[\"paper_part\"] = filename\n",
    "    temp = temp.rename(columns={\"StartPos\": \"Start\", \"CandidateCUI\":\"CUI\"})\n",
    "    temp[\"Start\"] = temp[\"Start\"] + start_idx\n",
    "    temp[\"End\"] = temp[\"Start\"] + temp[\"Length\"]\n",
    "    temp[\"Entity_matched\"] = temp.apply(lambda row: full_text[row['Start']:row['End']], axis=1)\n",
    "    temp = temp.rename(columns={'CandidateMatched':'Entity'})\n",
    "    temp = temp.drop([\"id\", \"Length\"], axis=1)\n",
    "    temp[\"Sentence_pred\"] = temp.apply(lambda row: extract_sentence(doc.sents, row), axis=1)\n",
    "\n",
    "    for i, row in temp.iterrows():\n",
    "        preds_csv_writer.writerow(list(row))\n",
    "\n",
    "        \n",
    "# analyze previous paper\n",
    "full_text = unidecode(full_text)\n",
    "\n",
    "with open(os.path.join(METAMAP_FULL_TEXT_DIRECTORY, papers_analyzed[-1]), \"w\") as f:\n",
    "    f.write(full_text)\n",
    "\n",
    "doc = nlp(full_text)\n",
    "matches = matcher(doc)\n",
    "spans = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    spans.append(span)\n",
    "\n",
    "filtered = spacy.util.filter_spans(spans) # use longest match\n",
    "\n",
    "for span in filtered:\n",
    "    row = [span.text, span.text.lower().strip(), papers_analyzed[-1], span.start_char, span.end_char, span.sent.text]\n",
    "    labels_csv_writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ABSTRACT:\n",
    "    for filename in metamap_tables:\n",
    "        print(filename)\n",
    "        paper = filename.split(\"_\")[0]\n",
    "        with open(os.path.join(METAMAP_TABLES_DIR, filename)) as f:\n",
    "            full_text = f.read()\n",
    "        full_text = unidecode(full_text)\n",
    "\n",
    "        doc = nlp(full_text)\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            spans.append(span)\n",
    "\n",
    "        filtered = spacy.util.filter_spans(spans) # use longest match\n",
    "\n",
    "        for span in filtered:\n",
    "            row = [span.text, span.text.lower().strip(), paper, span.start_char, span.end_char, span.sent.text]\n",
    "            labels_csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file.close()\n",
    "preds_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20408 papers analyzed by MetaMap\n"
     ]
    }
   ],
   "source": [
    "print(len(papers_analyzed), \"papers analyzed by MetaMap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_metamap_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
